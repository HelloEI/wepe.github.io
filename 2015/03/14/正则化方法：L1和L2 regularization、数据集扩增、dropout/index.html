<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>正则化方法：L1和L2 regularization、数据集扩增、dropout | wepon</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本文是《Neural networks and deep learning》概览 中第三章的一部分，讲机器学习/深度学习算法中常用的正则化方法。（本文会不断补充）

正则化方法：防止过拟合，提高泛化能力在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在training data上的error">
<meta property="og:type" content="article">
<meta property="og:title" content="正则化方法：L1和L2 regularization、数据集扩增、dropout">
<meta property="og:url" content="http://2hwp.com/2015/03/14/正则化方法：L1和L2 regularization、数据集扩增、dropout/index.html">
<meta property="og:site_name" content="wepon">
<meta property="og:description" content="本文是《Neural networks and deep learning》概览 中第三章的一部分，讲机器学习/深度学习算法中常用的正则化方法。（本文会不断补充）

正则化方法：防止过拟合，提高泛化能力在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在training data上的error">
<meta property="og:image" content="http://i.imgur.com/OhX9eFQ.jpg">
<meta property="og:image" content="http://i.imgur.com/9WnBBu1.jpg">
<meta property="og:image" content="http://i.imgur.com/mebEC90.jpg">
<meta property="og:image" content="http://i.imgur.com/qM83geg.jpg">
<meta property="og:image" content="http://i.imgur.com/Xs2p2EN.jpg">
<meta property="og:image" content="http://i.imgur.com/yDETU7x.jpg">
<meta property="og:image" content="http://i.imgur.com/RsR5cOK.png">
<meta property="og:image" content="http://i.imgur.com/6jbxq15.jpg">
<meta property="og:image" content="http://i.imgur.com/kju5RTZ.jpg">
<meta property="og:image" content="http://i.imgur.com/HCkJZYl.jpg">
<meta property="og:image" content="http://i.imgur.com/ZQvkoMn.png">
<meta property="og:image" content="http://i.imgur.com/G9QTbi9.png">
<meta property="og:image" content="http://i.imgur.com/VHRFgh6.png">
<meta property="og:updated_time" content="2016-02-03T08:09:53.923Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="正则化方法：L1和L2 regularization、数据集扩增、dropout">
<meta name="twitter:description" content="本文是《Neural networks and deep learning》概览 中第三章的一部分，讲机器学习/深度学习算法中常用的正则化方法。（本文会不断补充）

正则化方法：防止过拟合，提高泛化能力在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在training data上的error">
  
    <link rel="alternative" href="/atom.xml" title="wepon" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="http://7u2mpb.com1.z0.glb.clouddn.com/未名寒冬0.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">wepon</a></h1>
		</hgroup>

		

		
			<div class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/archives">文章归档</a></li>
				        
							<li><a href="/categories/机器学习">机器学习</a></li>
				        
							<li><a href="/categories/深度学习">深度学习</a></li>
				        
							<li><a href="/categories/计算机视觉">计算机视觉</a></li>
				        
							<li><a href="/categories/数据挖掘">数据挖掘</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
								<a class="mail" target="_blank" href="http://blog.csdn.net/u012162613" title="mail">mail</a>
					        
								<a class="github" target="_blank" href="https://github.com/wepe" title="github">github</a>
					        
								<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/wepon-huang" title="zhihu">zhihu</a>
					        
						</div>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/KMeans/" style="font-size: 10px;">KMeans</a> <a href="/tags/L1/" style="font-size: 10px;">L1</a> <a href="/tags/L2-regularization/" style="font-size: 10px;">L2 regularization</a> <a href="/tags/OpenCV/" style="font-size: 10px;">OpenCV</a> <a href="/tags/dropout/" style="font-size: 10px;">dropout</a> <a href="/tags/kaggle/" style="font-size: 10px;">kaggle</a> <a href="/tags/keras/" style="font-size: 20px;">keras</a> <a href="/tags/liblinear/" style="font-size: 10px;">liblinear</a> <a href="/tags/libsvm/" style="font-size: 10px;">libsvm</a> <a href="/tags/minibatch-size/" style="font-size: 10px;">minibatch size</a> <a href="/tags/theano/" style="font-size: 20px;">theano</a> <a href="/tags/二值化/" style="font-size: 10px;">二值化</a> <a href="/tags/人脸检测/" style="font-size: 10px;">人脸检测</a> <a href="/tags/伯努利模型/" style="font-size: 10px;">伯努利模型</a> <a href="/tags/可视化/" style="font-size: 20px;">可视化</a> <a href="/tags/多项式模型/" style="font-size: 10px;">多项式模型</a> <a href="/tags/学习速率/" style="font-size: 10px;">学习速率</a> <a href="/tags/朴素贝叶斯/" style="font-size: 10px;">朴素贝叶斯</a> <a href="/tags/标准化/" style="font-size: 10px;">标准化</a> <a href="/tags/正则化/" style="font-size: 10px;">正则化</a> <a href="/tags/正则项系数/" style="font-size: 10px;">正则项系数</a> <a href="/tags/流形学习/" style="font-size: 10px;">流形学习</a> <a href="/tags/类别特征转化/" style="font-size: 10px;">类别特征转化</a> <a href="/tags/聚类/" style="font-size: 10px;">聚类</a> <a href="/tags/规范化/" style="font-size: 10px;">规范化</a> <a href="/tags/贝叶斯定理/" style="font-size: 10px;">贝叶斯定理</a> <a href="/tags/降维/" style="font-size: 10px;">降维</a> <a href="/tags/高斯模型/" style="font-size: 10px;">高斯模型</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">PKU在读，机器学习相关的都喜欢，欢迎收藏本站，我不定期会写写知识总结。个人邮箱不贴上来了，因为之前每天收到很多邮件，我最近没有足够的精力去一一回复，实在抱歉！关于代码问题，可以到Github上提issue，非常感谢。</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide">wepon</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="http://7u2mpb.com1.z0.glb.clouddn.com/未名寒冬0.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author">wepon</h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/archives">文章归档</a></li>
		        
					<li><a href="/categories/机器学习">机器学习</a></li>
		        
					<li><a href="/categories/深度学习">深度学习</a></li>
		        
					<li><a href="/categories/计算机视觉">计算机视觉</a></li>
		        
					<li><a href="/categories/数据挖掘">数据挖掘</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="mail" target="_blank" href="http://blog.csdn.net/u012162613" title="mail">mail</a>
			        
						<a class="github" target="_blank" href="https://github.com/wepe" title="github">github</a>
			        
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/wepon-huang" title="zhihu">zhihu</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-正则化方法：L1和L2 regularization、数据集扩增、dropout" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/03/14/正则化方法：L1和L2 regularization、数据集扩增、dropout/" class="article-date">
  	<time datetime="2015-03-13T16:00:00.000Z" itemprop="datePublished">2015-03-14</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      正则化方法：L1和L2 regularization、数据集扩增、dropout
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/L1/">L1</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/L2-regularization/">L2 regularization</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/dropout/">dropout</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/正则化/">正则化</a></li></ul>
	</div>

        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/机器学习/">机器学习</a>
	</div>


        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文是<a href="http://blog.csdn.net/u012162613/article/details/44220115" target="_blank" rel="external">《Neural networks and deep learning》概览</a> 中第三章的一部分，讲机器学习/深度学习算法中常用的正则化方法。（本文会不断补充）</p>
<hr>
<h2 id="正则化方法：防止过拟合，提高泛化能力"><strong>正则化方法：防止过拟合，提高泛化能力</strong></h2><p>在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集，对训练集外的数据却不work。</p>
<p><img src="http://i.imgur.com/OhX9eFQ.jpg" alt=""></p>
<a id="more"></a>
<p>为了防止overfitting，可以用的方法有很多，下文就将以此展开。有一个概念需要先说明，在机器学习算法中，我们常常将原始数据集分为三部分：training data、validation data，testing data。这个validation data是什么？它其实就是用来避免过拟合的，在训练过程中，我们通常用它来确定一些超参数（比如根据validation data上的accuracy来确定early stopping的epoch大小、根据validation data确定learning rate等等）。那为啥不直接在testing data上做这些呢？因为如果在testing data做这些，那么随着训练的进行，我们的网络实际上就是在一点一点地overfitting我们的testing data，导致最后得到的testing accuracy没有任何参考意义。因此，training data的作用是计算梯度更新权重，validation data如上所述，testing data则给出一个accuracy以判断网络的好坏。</p>
<p>避免过拟合的方法有很多：early stopping、数据集扩增（Data augmentation）、正则化（Regularization）包括L1、L2（L2 regularization也叫weight decay），dropout。</p>
<!--more-->
<hr>
<h2 id="L2_regularization（权重衰减）"><strong>L2 regularization（权重衰减）</strong></h2><p>L2正则化就是在代价函数后面再加上一个正则化项：</p>
<p><img src="http://i.imgur.com/9WnBBu1.jpg" alt=""></p>
<p>C0代表原始的代价函数，后面那一项就是L2正则化项，它是这样来的：所有参数w的平方的和，除以训练集的样本大小n。λ就是正则项系数，权衡正则项与C0项的比重。另外还有一个系数1/2，1/2经常会看到，主要是为了后面求导的结果方便，后面那一项求导会产生一个2，与1/2相乘刚好凑整。</p>
<p>L2正则化项是怎么避免overfitting的呢？我们推导一下看看，先求导：</p>
<p><img src="http://i.imgur.com/mebEC90.jpg" alt=""></p>
<p>可以发现L2正则化项对b的更新没有影响，但是对于w的更新有影响:</p>
<p><img src="http://i.imgur.com/qM83geg.jpg" alt=""></p>
<p>在不使用L2正则化时，求导结果中w前系数为1，现在w前面系数为 1−ηλ/n ，因为η、λ、n都是正的，所以 1−ηλ/n小于1，它的效果是减小w，这也就是权重衰减（weight decay）的由来。当然考虑到后面的导数项，w最终的值可能增大也可能减小。</p>
<p>另外，需要提一下，对于基于mini-batch的随机梯度下降，w和b更新的公式跟上面给出的有点不同：</p>
<p><img src="http://i.imgur.com/Xs2p2EN.jpg" alt=""></p>
<p><img src="http://i.imgur.com/yDETU7x.jpg" alt=""></p>
<p>对比上面w的更新公式，可以发现后面那一项变了，变成所有导数加和，乘以η再除以m，m是一个mini-batch中样本的个数。</p>
<p>到目前为止，我们只是解释了L2正则化项有让w“变小”的效果，但是还没解释为什么w“变小”可以防止overfitting？一个所谓“显而易见”的解释就是：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。当然，对于很多人（包括我）来说，这个解释似乎不那么显而易见，所以这里添加一个稍微数学一点的解释（引自知乎）：</p>
<p>过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。</p>
<p><img src="http://i.imgur.com/RsR5cOK.png" alt=""></p>
<p>而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。</p>
<hr>
<h2 id="L1_regularization"><strong>L1 regularization</strong></h2><p>在原始的代价函数后面加上一个L1正则化项，即所有权重w的绝对值的和，乘以λ/n（这里不像L2正则化项那样，需要再乘以1/2，具体原因上面已经说过。）</p>
<p><img src="http://i.imgur.com/6jbxq15.jpg" alt=""></p>
<p>同样先计算导数：</p>
<p><img src="http://i.imgur.com/kju5RTZ.jpg" alt=""></p>
<p>上式中sgn(w)表示w的符号。那么权重w的更新规则为：</p>
<p><img src="http://i.imgur.com/HCkJZYl.jpg" alt=""></p>
<p>比原始的更新规则多出了η <em> λ </em> sgn(w)/n这一项。当w为正时，更新后的w变小。当w为负时，更新后的w变大——因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。</p>
<p>另外，上面没有提到一个问题，当w为0时怎么办？当w等于0时，|W|是不可导的，所以我们只能按照原始的未经正则化的方法去更新w，这就相当于去掉η<em>λ</em>sgn(w)/n这一项，所以我们可以规定sgn(0)=0，这样就把w=0的情况也统一进来了。（在编程的时候，令sgn(0)=0,sgn(w&gt;0)=1,sgn(w&lt;0)=-1）</p>
<hr>
<h2 id="Dropout"><strong>Dropout</strong></h2><p>L1、L2正则化是通过修改代价函数来实现的，而Dropout则是通过修改神经网络本身来实现的，它是在训练网络时用的一种技巧（trike）。它的流程如下：</p>
<p><img src="http://i.imgur.com/ZQvkoMn.png" alt=""></p>
<p>假设我们要训练上图这个网络，在训练开始时，我们随机地“删除”一半的隐层单元，视它们为不存在，得到如下的网络：</p>
<p><img src="http://i.imgur.com/G9QTbi9.png" alt=""></p>
<p>保持输入输出层不变，按照BP算法更新上图神经网络中的权值（虚线连接的单元不更新，因为它们被“临时删除”了）。</p>
<p>以上就是一次迭代的过程，在第二次迭代中，也用同样的方法，只不过这次删除的那一半隐层单元，跟上一次删除掉的肯定是不一样的，因为我们每一次迭代都是“随机”地去删掉一半。第三次、第四次……都是这样，直至训练结束。</p>
<p>以上就是Dropout，它为什么有助于防止过拟合呢？可以简单地这样解释，运用了dropout的训练过程，相当于训练了很多个只有半数隐层单元的神经网络（后面简称为“半数网络”），每一个这样的半数网络，都可以给出一个分类结果，这些结果有的是正确的，有的是错误的。随着训练的进行，大部分半数网络都可以给出正确的分类结果，那么少数的错误分类结果就不会对最终结果造成大的影响。</p>
<p>更加深入地理解，可以看看Hinton和Alex两牛2012的论文《ImageNet Classification with Deep Convolutional Neural Networks》</p>
<hr>
<h2 id="数据集扩增（data_augmentation）"><strong>数据集扩增（data augmentation）</strong></h2><p>“有时候不是因为算法好赢了，而是因为拥有更多的数据才赢了。”</p>
<p>不记得原话是哪位大牛说的了，hinton？从中可见训练数据有多么重要，特别是在深度学习方法中，更多的训练数据，意味着可以用更深的网络，训练出更好的模型。</p>
<p>既然这样，收集更多的数据不就行啦？如果能够收集更多可以用的数据，当然好。但是很多时候，收集更多的数据意味着需要耗费更多的人力物力，有弄过人工标注的同学就知道，效率特别低，简直是粗活。</p>
<p>所以，可以在原始数据上做些改动，得到更多的数据，以图片数据集举例，可以做各种变换，如：</p>
<ul>
<li><p>将原始图片旋转一个小角度</p>
</li>
<li><p>添加随机噪声</p>
</li>
<li><p>一些有弹性的畸变（elastic distortions），论文《Best practices for convolutional neural networks applied to visual document analysis》对MNIST做了各种变种扩增。</p>
</li>
<li><p>截取（crop）原始图片的一部分。比如DeepID中，从一副人脸图中，截取出了100个小patch作为训练数据，极大地增加了数据集。感兴趣的可以看《Deep learning face representation from predicting 10,000 classes》.</p>
</li>
</ul>
<p> <em>更多数据意味着什么？</em></p>
<p>用50000个MNIST的样本训练SVM得出的accuracy94.48%，用5000个MNIST的样本训练NN得出accuracy为93.24%，所以更多的数据可以使算法表现得更好。在机器学习中，算法本身并不能决出胜负，不能武断地说这些算法谁优谁劣，因为数据对算法性能的影响很大。</p>
<p><img src="http://i.imgur.com/VHRFgh6.png" alt=""></p>
<hr>
<p>转载请注明出处：<a href="http://blog.csdn.net/u012162613/article/details/44261657" target="_blank" rel="external">http://blog.csdn.net/u012162613/article/details/44261657</a></p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2015/03/24/Kaggle-Otto-Group-Product-Classification/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption"><</strong>
      <div class="article-nav-title">
        
          Kaggle比赛-Otto Group Product Classification
        
      </div>
    </a>
  
  
    <a href="/2015/03/14/机器学习算法中如何选取超参数：学习速率、正则项系数、minibatch size/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">机器学习算法中如何选取超参数：学习速率、正则项系数、minibatch size</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>


<div class="share">
	<!-- JiaThis Button BEGIN -->
	<div class="jiathis_style">
		<span class="jiathis_txt">分享到：</span>
		<a class="jiathis_button_tsina"></a>
		<a class="jiathis_button_cqq"></a>
		<a class="jiathis_button_douban"></a>
		<a class="jiathis_button_weixin"></a>
		<a class="jiathis_button_tumblr"></a>
		<a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jtico jtico_jiathis" target="_blank"></a>
	</div>
	<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=1405949716054953" charset="utf-8"></script>
	<!-- JiaThis Button END -->
</div>





</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2016 wepon
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    
  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">


<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>
<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  </div>
</body>
</html>